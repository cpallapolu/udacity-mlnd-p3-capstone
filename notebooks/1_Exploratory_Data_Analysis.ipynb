{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Revenue Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "*Machine Learning Nanodegree Program | Capstone Project*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be exploring the dataset, noting patterns about the data points and distribution of the dataset.\n",
    "\n",
    "### Overview:\n",
    "- Reading the data\n",
    "- Filling in the missing data\n",
    "- Analyzing the dataset\n",
    "- Removing the constant value columns\n",
    "- Normalizing the dataset \n",
    "- Spliting the data into training, validation and test sets\n",
    "- Saving the files\n",
    "\n",
    "### Dataset:\n",
    "The dataset is download from the [Kaggle competetion](https://www.kaggle.com/c/ga-customer-revenue-prediction/overview) Google Analytics Customer Revenue Prediction.\n",
    "\n",
    "#### Initial Data:\n",
    "- fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n",
    "- channelGrouping - The channel via which the user came to the Store.\n",
    "- date - The date on which the user visited the Store.\n",
    "- sessionId - The sessionId of the user visited the Store.\n",
    "- device - The specifications for the device used to access the Store.\n",
    "- geoNetwork - This section contains information about the geography of the user.\n",
    "- socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n",
    "- totals - This section contains aggregate values across the session.\n",
    "- trafficSource - This section contains information about the Traffic Source from which the session originated.\n",
    "- visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n",
    "- visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n",
    "- visitStartTime - The timestamp (expressed as POSIX time).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the relevant libraries into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import squarify\n",
    "import plotly.graph_objs as go \n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from plotly.offline import iplot, plot \n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the various paths for the files that are used for reading and writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_dir = '../data/originals'\n",
    "\n",
    "if not path.exists(originals_dir):\n",
    "    raise Exception('{} directory not found.'.format(\n",
    "        originals_dir\n",
    "    ))\n",
    "\n",
    "input_train_file = '{}/{}'.format(originals_dir, 'train.zip')\n",
    "print('\\nInput Train file: {}'.format(input_train_file))\n",
    "\n",
    "input_test_file = '{}/{}'.format(originals_dir, 'test.zip')\n",
    "print('\\nInput Test file: {}'.format(input_test_file))\n",
    "\n",
    "files_dir = '../data/files'\n",
    "\n",
    "if not path.exists(files_dir):\n",
    "    os.makedirs(files_dir)\n",
    "\n",
    "train_file = '{}/{}'.format(files_dir, 'train.zip')\n",
    "print('\\nTrain file: {}'.format(train_file))\n",
    "\n",
    "val_file = '{}/{}'.format(files_dir, 'val.zip')\n",
    "print('\\nValidation file: {}'.format(val_file))\n",
    "\n",
    "test_file = '{}/{}'.format(files_dir, 'test.zip')\n",
    "print('\\nTest file: {}'.format(test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns in the intial data are JSON objects. We need to serialize them so as to access the various data points in the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        data_path,\n",
    "        converters={column: json.loads for column in JSON_COLUMNS},\n",
    "        dtype={'fullVisitorId': 'str'},\n",
    "        compression='zip'\n",
    "    )\n",
    "\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f'{column}.{subcolumn}' for subcolumn in column_as_df.columns]\n",
    "\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "        df = df.merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "    [rows, columns] = df.shape\n",
    "\n",
    "    print('Loaded {} rows with {} columns from {}.'.format(\n",
    "        rows, columns, data_path\n",
    "    ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "train_df = load_data(input_train_file)\n",
    "test_df = load_data(input_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the import and transformation, we have 55 columns.\n",
    "Now, lets see our data and handle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "total_null_sum = train_df.isnull().sum()\n",
    "total_null_count = train_df.isnull().count()\n",
    "\n",
    "train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].astype('float')\n",
    "\n",
    "dfgv_id = train_df.groupby('fullVisitorId')\n",
    "dfgv_id = dfgv_id['totals.transactionRevenue'].sum().reset_index()\n",
    "\n",
    "total = total_null_sum.sort_values(ascending=False)\n",
    "percent = (total_null_sum / total_null_count * 100)\n",
    "percent = percent.sort_values(ascending=False)\n",
    "\n",
    "df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "rows = train_df.shape[0]\n",
    "uniq_rows = dfgv_id.shape[0]\n",
    "uniq_visitors = train_df['fullVisitorId'].nunique()\n",
    "    \n",
    "nzi = pd.notnull(train_df[\"totals.transactionRevenue\"]).sum()\n",
    "nzr = (dfgv_id['totals.transactionRevenue'] > 0).sum()\n",
    "\n",
    "print('\\n{} unique customers in train set. Ratio: {}.'.format(uniq_visitors, (uniq_visitors / rows)))\n",
    "print('{} instances have non-zero revenue out of {}. Ratio: {}.'.format(nzi, rows, (nzi / rows)))\n",
    "print('{} unique customers with non-zero revenue out of {}. Ratio: {}.'.format(nzr, uniq_rows, (nzr / uniq_rows)))\n",
    "print('{} number of common visitors in train and test set.'.format(\n",
    "    len(set(train_df['fullVisitorId'].unique()).intersection(set(test_df['fullVisitorId'].unique())))\n",
    "))\n",
    "\n",
    "print('\\nTotal columns with at least one Values:')\n",
    "print(df[~(df['Total'] == 0)])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we see that some columns have null values. We will be defaulting the values for the columns that have less than 90% null values and remove the columns that have more than 90% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOriginal Dataframe Info:\\n')\n",
    "\n",
    "print(train_df.info())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "na_val_keys = {\n",
    "    'totals.pageviews': [1, 'int'],\n",
    "    'totals.newVisits': [0, 'int'],\n",
    "    'totals.bounces': [0, 'int'],\n",
    "    'totals.transactionRevenue': [0, 'float'],\n",
    "    'trafficSource.isTrueDirect': [False, 'bool'],\n",
    "    'trafficSource.referralPath': ['N/A', 'str']\n",
    "}\n",
    "\n",
    "for nav_k, [nav_v, nav_t] in na_val_keys.items():\n",
    "    train_df[nav_k] = train_df[nav_k].fillna(nav_v)\n",
    "    train_df[nav_k] = train_df[nav_k].astype(nav_t)\n",
    "    \n",
    "    if nav_k != 'totals.transactionRevenue':\n",
    "        test_df[nav_k] = test_df[nav_k].fillna(nav_v)\n",
    "        test_df[nav_k] = test_df[nav_k].astype(nav_t)\n",
    "    \n",
    "na_vals = [\n",
    "    'unknown.unknown', '(not set)', 'not available in demo dataset', \n",
    "    '(not provided)', '(none)', '<NA>', 'nan'\n",
    "]\n",
    "\n",
    "for na_val in na_vals:\n",
    "    train_df = train_df.replace(na_val, 'N/A')\n",
    "    test_df = test_df.replace(na_val, 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some columns that have unique values which may not be useful in our model training. So we need to remove those columns along with the columns that have 90% of values as null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "const_cols = [\n",
    "    c\n",
    "    for c in train_df.columns\n",
    "    if train_df[c].nunique(dropna=False) == 1\n",
    "]\n",
    "\n",
    "const_cols += [\n",
    "    'sessionId', 'visitId', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot',\n",
    "    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.gclId',\n",
    "    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.isVideoAd',\n",
    "]\n",
    "\n",
    "cols_not_in_test = set(train_df.columns).difference(set(test_df.columns))\n",
    "\n",
    "train_const_cols = const_cols + ['trafficSource.campaignCode']\n",
    "\n",
    "train_df.drop(train_const_cols, axis=1, inplace=True)\n",
    "test_df.drop(const_cols, axis=1, inplace=True)\n",
    "\n",
    "print('\\nTotal Train Features dropped : {}'.format(train_const_cols))\n",
    "print('\\nTrain features dropped: {}'.format(len(train_const_cols)))\n",
    "\n",
    "print('\\nTotal Test Features dropped : {}'.format(const_cols))\n",
    "print('\\nTest features dropped: {}'.format(len(const_cols)))\n",
    "\n",
    "print('\\nTrain Shape after dropping: {}'.format(train_df.shape))\n",
    "print('\\nTest Shape after dropping: {}\\n'.format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize some patterns between the various features.\n",
    "<br />\n",
    "<div align=\"right\">\n",
    "    <font size=\"2\">\n",
    "        Used the visualizations from <a href=\"https://www.kaggle.com/kabure/exploring-the-consumer-patterns-ml-pipeline\">here</a> \n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenue Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "ax = sns.distplot(\n",
    "    np.log1p(train_df[train_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"] + 0.01), \n",
    "    bins=40, \n",
    "    kde=True\n",
    ")\n",
    "ax.set_xlabel('Transaction RevenueLog', fontsize=12) \n",
    "ax.set_ylabel('Distribuition', fontsize=12) \n",
    "ax.set_title(\"Distribuition of Revenue Log\", fontsize=12) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df['totals.transactionRevenue'].values))\n",
    "plt.xlabel('Index', fontsize=12)\n",
    "plt.ylabel('Revenue value', fontsize=12) \n",
    "plt.title(\"Revenue Value Distribution\", fontsize=12) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossing Channel Grouping x Browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_eda = pd.crosstab(\n",
    "    index=train_df['channelGrouping'], normalize=True,\n",
    "    columns=train_df[train_df['device.browser'].isin(train_df['device.browser'].value_counts()[:5].index.values)]['device.browser']\n",
    ")\n",
    "\n",
    "crosstab_eda.plot(\n",
    "    kind=\"bar\",    \n",
    "    figsize=(14,6),\n",
    "    stacked=True\n",
    ")\n",
    "\n",
    "plt.title(\"Channel Grouping % for which Browser\", fontsize=12) \n",
    "plt.xlabel(\"The Channel Grouping Name\", fontsize=12) \n",
    "plt.ylabel(\"Count\", fontsize=12) \n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossing Operation Systems x Browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_eda = pd.crosstab(\n",
    "    index=train_df[train_df['device.operatingSystem'].isin(train_df['device.operatingSystem'].value_counts()[:6].index.values)]['device.operatingSystem'], \n",
    "    columns=train_df[train_df['device.browser'].isin(train_df['device.browser'].value_counts()[:5].index.values)]['device.browser'])\n",
    "\n",
    "crosstab_eda.plot(\n",
    "    kind=\"bar\",    \n",
    "    figsize=(14,6), \n",
    "    stacked=True\n",
    ")   \n",
    "\n",
    "plt.title(\"Most frequent OS's by Browsers of users\", fontsize=12) \n",
    "plt.xlabel(\"Operational System Name\", fontsize=12) \n",
    "plt.ylabel(\"Count OS\", fontsize=12) \n",
    "plt.xticks(rotation=0) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossing Device Catgory x Operating Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_eda = pd.crosstab(\n",
    "    index=train_df['device.deviceCategory'], \n",
    "    columns=train_df[train_df['device.operatingSystem'].isin(train_df['device.operatingSystem'].value_counts()[:6].index.values)]['device.operatingSystem']\n",
    ")\n",
    "\n",
    "crosstab_eda.plot(\n",
    "    kind=\"bar\",    \n",
    "    figsize=(14,6), \n",
    "    stacked=True\n",
    ")   \n",
    "plt.title(\"Most frequent OS's by Device Categorys of users\", fontsize=12) \n",
    "plt.xlabel(\"Device Name\", fontsize=12)                \n",
    "plt.ylabel(\"Count Device x OS\", fontsize=12)          \n",
    "plt.xticks(rotation=0)                                \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_colors = 20 \n",
    "\n",
    "color = [\n",
    "    \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "    for i in range(number_of_colors)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_tree = train_df[\"geoNetwork.country\"].value_counts() \n",
    "\n",
    "print(\"Description most frequent countrys: \")\n",
    "print(country_tree[:15])\n",
    "\n",
    "country_tree = round(\n",
    "    (train_df[\"geoNetwork.country\"].value_counts()[:30] / len(train_df['geoNetwork.country']) * 100),\n",
    "    2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "g = squarify.plot(\n",
    "    sizes=country_tree.values, \n",
    "    label=country_tree.index, \n",
    "    value=country_tree.values,\n",
    "    alpha=.4, \n",
    "    color=color\n",
    ")\n",
    "g.set_title(\"'TOP 30 Countrys - % size of total\",fontsize=12)\n",
    "g.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_tree = train_df[\"geoNetwork.city\"].value_counts()\n",
    "\n",
    "print(\"Description most frequent Citys: \" )\n",
    "print(city_tree[:15])\n",
    "\n",
    "city_tree = round(\n",
    "    (train_df[\"geoNetwork.city\"].value_counts()[:30] / len(train_df['geoNetwork.city']) * 100),\n",
    "    2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "g = squarify.plot(\n",
    "    sizes=city_tree.values, \n",
    "    label=city_tree.index, \n",
    "    value=city_tree.values,\n",
    "    alpha=.4, \n",
    "    color=color\n",
    ")\n",
    "\n",
    "g.set_title(\"'TOP 30 Citys - % size of total\",fontsize=12)\n",
    "g.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PieChart(df_colum, title, limit=15):\n",
    "    count_trace = train_df[df_colum].value_counts()[:limit].to_frame().reset_index()\n",
    "    rev_trace = train_df.groupby(df_colum)[\"totals.transactionRevenue\"].sum().nlargest(10).to_frame().reset_index()\n",
    "\n",
    "    trace1 = go.Pie(\n",
    "        labels=count_trace['index'], \n",
    "        values=count_trace[df_colum], \n",
    "        name= \"% Acesses\", \n",
    "        hole= .5, \n",
    "        hoverinfo=\"label+percent+name\", \n",
    "        showlegend=True,\n",
    "        domain= {'x': [0, .48]}, \n",
    "        marker=dict(colors=color)\n",
    "    )\n",
    "\n",
    "    trace2 = go.Pie(\n",
    "        labels=rev_trace[df_colum], \n",
    "        values=rev_trace['totals.transactionRevenue'], \n",
    "        name=\"% Revenue\", \n",
    "        hole= .5, \n",
    "        hoverinfo=\"label+percent+name\", \n",
    "        showlegend=False, \n",
    "        domain= {'x': [.52, 1]}\n",
    "    )\n",
    "\n",
    "    layout = dict(\n",
    "        title= title, \n",
    "        height=450, \n",
    "        font=dict(size=15),\n",
    "        annotations = [\n",
    "            dict(x=.25, y=.5, text='Visits', showarrow=False, font=dict(size=20)),\n",
    "            dict(x=.80, y=.5, text='Revenue', showarrow=False, font=dict(size=20))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig = dict(data=[trace1, trace2], layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Category feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PieChart(\"device.deviceCategory\", \"Device Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Borwser feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PieChart(\"device.browser\", \"Device Browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating System feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PieChart(\"device.operatingSystem\", \"Operating System\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us encode the features an normilize them so that the values are between 0 and 1. I will be using LabelEncoder from sklearn to encode the values. Then use MinMaxScaler to normalize the feature values between 0 and 1.\n",
    "\n",
    "Inorder to not to contaminate the test features, I will be using the separate instance on MinMaxScaler for train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = [\n",
    "    'channelGrouping', 'device.browser', 'device.deviceCategory', 'device.isMobile', \n",
    "    'device.operatingSystem', 'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', \n",
    "    'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent', \n",
    "    'trafficSource.campaign', 'trafficSource.isTrueDirect', 'trafficSource.keyword', \n",
    "    'trafficSource.medium', 'trafficSource.source', 'trafficSource.referralPath'\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'visitNumber', 'visitStartTime'\n",
    "]\n",
    "\n",
    "train_core_df = train_core_df = train_df.loc[:, ['fullVisitorId', 'totals.transactionRevenue', 'date']]\n",
    "test_core_df = test_df.loc[:, ['fullVisitorId', 'date']]\n",
    "\n",
    "train_rest_df = train_df.loc[:,   str_cols + num_cols]\n",
    "test_rest_df = test_df.loc[:, str_cols + num_cols]\n",
    "\n",
    "for str_col in str_cols:\n",
    "    labelEncoder = LabelEncoder()\n",
    "    \n",
    "    train_col_list = list(train_rest_df[str_col].astype('str'))\n",
    "    test_col_list = list(test_rest_df[str_col].astype('str'))\n",
    "    \n",
    "    labelEncoder.fit(train_col_list + test_col_list)\n",
    "    \n",
    "    train_rest_df[str_col] = labelEncoder.transform(train_col_list)\n",
    "    test_rest_df[str_col] = labelEncoder.transform(test_col_list)\n",
    "\n",
    "for num_col in num_cols:\n",
    "    train_rest_df[num_col] = train_rest_df[num_col].astype('float')\n",
    "    test_rest_df[num_col] = test_rest_df[num_col].astype('float')\n",
    "    \n",
    "train_MinMaxScaler = MinMaxScaler()\n",
    "\n",
    "normalized_train_df = pd.DataFrame(train_MinMaxScaler.fit_transform(train_rest_df.astype('float')))\n",
    "normalized_train_df.columns = train_rest_df.columns\n",
    "normalized_train_df.index = train_rest_df.index\n",
    "\n",
    "test_MinMaxScaler = MinMaxScaler()\n",
    "\n",
    "normalized_test_df = pd.DataFrame(test_MinMaxScaler.fit_transform(test_rest_df.astype('float')))\n",
    "normalized_test_df.columns = test_rest_df.columns\n",
    "normalized_test_df.index = test_rest_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_df = pd.concat([train_core_df, normalized_train_df], axis=1)\n",
    "\n",
    "cleaned_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_df = pd.concat([test_core_df, normalized_test_df], axis=1)\n",
    "\n",
    "cleaned_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transactions Minimum Date: {}'.format(cleaned_train_df['date'].min()))\n",
    "print('Transactions Maximum Date: {}\\n'.format(cleaned_train_df['date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have normalized the data we need to split the dataset into development, validation. I will be splitting the data into \n",
    "- dev: 85% i.e, date ranges between 20160801 - 20170531\n",
    "- val: 15% i.e. date ranges between 20170601 - 20170801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_df[\"date\"] = cleaned_train_df[\"date\"].astype('str').apply(lambda x: parse(x, yearfirst=True))\n",
    "\n",
    "dev_df = cleaned_train_df[cleaned_train_df['date'] < datetime(2017, 6, 1)]\n",
    "val_df = cleaned_train_df[cleaned_train_df['date'] >= datetime(2017, 6, 1)]\n",
    "\n",
    "feature_cols = ['fullVisitorId'] + str_cols + num_cols\n",
    "\n",
    "dev_X = dev_df[['totals.transactionRevenue'] + feature_cols]\n",
    "val_X = val_df[['totals.transactionRevenue'] + feature_cols]\n",
    "test_X = cleaned_test_df[feature_cols]\n",
    "\n",
    "print('\\nNumber of instances in train: {} with {} columns.'.format(dev_X.shape[0], dev_X.shape[1]))\n",
    "print('\\nNumber of instances in val: {} with {} columns.'.format(val_X.shape[0], val_X.shape[1]))\n",
    "print('\\nNumber of instances in test: {} with {} columns.'.format(test_X.shape[0], test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write these files in csv format so that we can use them in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X.to_csv(train_file, index=False, compression='zip')\n",
    "val_X.to_csv(val_file, index=False, compression='zip')\n",
    "test_X.to_csv(test_file, index=False, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
